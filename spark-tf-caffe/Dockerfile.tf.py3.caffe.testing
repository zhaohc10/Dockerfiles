# Start with Ubuntu base image
FROM ubuntu:14.04
MAINTAINER Huichao Zhao
##############################
# Install basic dependencies
##############################
RUN apt-get update && apt-get install -y \
  wget \
  vim \
  unzip \
  git \
  bc \
  cmake \
  libatlas-base-dev \
  libatlas-dev \
  libboost-all-dev \
  libopencv-dev \
  libprotobuf-dev \
  libgoogle-glog-dev \
  libgflags-dev \
  protobuf-compiler \
  libhdf5-dev \
  libleveldb-dev \
  liblmdb-dev \
  libsnappy-dev \
  python3.4 \
  python3-setuptools \
  python3-dev \
  python3-pip \
  python3-scipy \
  python3-matplotlib \
  python3-pandas \
  gfortran > /dev/null

RUN pip3 install scikit-learn
##############################
#   Spark dependencies
##############################
ENV APACHE_SPARK_VERSION 1.6.0
RUN apt-get -y update && \
    apt-get install -y --no-install-recommends openjdk-7-jre-headless && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
RUN cd /tmp && \
        wget -q http://d3kbcqa49mib13.cloudfront.net/spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz && \
        echo "439fe7793e0725492d3d36448adcd1db38f438dd1392bffd556b58bb9a3a2601 *spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz" | sha256sum -c - && \
        tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz -C /usr/local && \
        rm spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz && \
        rm /usr/local/spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6/lib/spark-examples-*.jar
RUN cd /usr/local && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6 spark

##############################
# Mesos dependencies
##############################
RUN apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF && \
    DISTRO=debian && \
    CODENAME=wheezy && \
    echo "deb http://repos.mesosphere.io/${DISTRO} ${CODENAME} main" > /etc/apt/sources.list.d/mesosphere.list && \
    apt-get -y update && \
    apt-get --no-install-recommends -y --force-yes install mesos=0.22.1-1.0.debian78 && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
##############################
# Spark and Mesos config
##############################
ENV SPARK_HOME /usr/local/spark
ENV PYSPARK_PYTHON python3.4
ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.9-src.zip
ENV MESOS_NATIVE_LIBRARY /usr/local/lib/libmesos.so
ENV SPARK_OPTS --driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info
ENV PATH $PATH:/usr/local/spark/bin

##############################
# TensorFlow
##############################
# TensorFlow Ubuntu/Linux 64-bit, CPU only:
RUN sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl

# TensorFlow Ubuntu/Linux 64-bit, GPU enabled:
# RUN sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl

##############################
#   Install Caffe
##############################

# Clone Caffe repo and move into it
RUN cd /root && git clone https://github.com/BVLC/caffe.git && cd caffe && \
# Install python dependencies
  cat python/requirements.txt | xargs -n1 pip3 install
# Move into Caffe repo
RUN cd /root/caffe && \
# Make and move into build directory
  mkdir build && cd build && \
# CMake
  cmake .. && \
# Make
  make -j"$(nproc)" all && \
  make install

# Add to Python path
ENV PYTHONPATH=/root/caffe/python:$PYTHONPATH
##############################
# Autoshift related
##############################
# Install pykafka
RUN wget https://github.com/Parsely/pykafka/archive/master.zip
RUN unzip master.zip
RUN cd pykafka-master; python3 setup.py install

# autoshift dependenices Script
#ADD code/*.py /
#ADD scripts/*.sh /
#RUN sudo chmod +x /*.py && sudo chmod +x /pyspark-submit.sh

# Empty kakfa msg data dir, so that our init script can start from a clean slate
RUN rm -rf /var/data/pysparkoutput/*

# Define mountable directories.
VOLUME ["/var/data/kafka/pysparkoutput"]
##############################
CMD ["/pyspark-submit.sh"]

